#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞
–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–æ–µ–∫—Ç–∞ Email Intelligence Collector
"""

import asyncio
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append('/Users/rafaelamirov/Documents/metod/Email-Intelligence-Collector/backend')

from modules.web_scraper import EnhancedWebScraper, ScrapingConfig, SelectorConfig

async def demo_enhanced_scraper():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞"""
    
    print("=" * 60)
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –£–õ–£–ß–®–ï–ù–ù–û–ì–û –í–ï–ë-–°–ö–†–ê–ü–ï–†–ê")
    print("=" * 60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = ScrapingConfig(
        max_pages=3,
        concurrent_requests=2,
        timeout=15,
        retry_attempts=2,
        cache_ttl=300,  # 5 –º–∏–Ω—É—Ç
        delay_between_requests=1.0
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
    selector_config = SelectorConfig()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ URL (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É–±–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏)
    test_urls = [
        "https://example.com",  # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
        "https://httpbin.org/html",  # HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        "https://httpbin.org/json",  # JSON –æ—Ç–≤–µ—Ç (–±—É–¥–µ—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ –Ω–µ HTML)
    ]
    
    email = "demo@example.com"
    
    print(f"üìß –¢–µ—Å—Ç–æ–≤—ã–π email: {email}")
    print(f"üîó URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(test_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü")
    print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: –º–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü={config.max_pages}, —Ç–∞–π–º–∞—É—Ç={config.timeout}—Å")
    print()
    
    try:
        async with EnhancedWebScraper(email, config, selector_config) as scraper:
            print("üîç –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞...")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥
            results = await scraper.scrape_websites_enhanced(test_urls)
            
            print("‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print()
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ö–†–ê–ü–ò–ù–ì–ê:")
            print("-" * 40)
            
            # –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            person_info = results.get('person_info', {})
            print(f"üë§ –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            if person_info:
                for key, value in person_info.items():
                    print(f"   {key}: {value}")
            else:
                print("   –ù–µ –Ω–∞–π–¥–µ–Ω–∞")
            print()
            
            # –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            contact_info = results.get('contact_info', {})
            print(f"üìû –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            print(f"   Email-–∞–¥—Ä–µ—Å–∞: {len(contact_info.get('emails', []))}")
            print(f"   –¢–µ–ª–µ—Ñ–æ–Ω—ã: {len(contact_info.get('phones', []))}")
            print(f"   –ê–¥—Ä–µ—Å–∞: {len(contact_info.get('addresses', []))}")
            print()
            
            # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            social_links = results.get('social_links', [])
            print(f"üîó –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏: {len(social_links)}")
            for link in social_links[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"   {link.get('platform', 'Unknown')}: {link.get('url', 'N/A')}")
            print()
            
            # NLP –∞–Ω–∞–ª–∏–∑
            nlp_analysis = results.get('nlp_analysis', {})
            print(f"üß† NLP –∞–Ω–∞–ª–∏–∑:")
            if nlp_analysis:
                for url, analysis in list(nlp_analysis.items())[:2]:  # –ü–µ—Ä–≤—ã–µ 2 URL
                    print(f"   URL: {url}")
                    sentiment = analysis.get('sentiment', {})
                    print(f"   –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –ø–æ–ª—è—Ä–Ω–æ—Å—Ç—å={sentiment.get('polarity', 0):.2f}, —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å={sentiment.get('subjectivity', 0):.2f}")
                    
                    entities = analysis.get('entities_spacy', {})
                    if entities:
                        print(f"   –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities.keys())}")
                    
                    lang = analysis.get('language_detected', 'unknown')
                    print(f"   –Ø–∑—ã–∫: {lang}")
                    print()
            else:
                print("   –ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            print()
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            content_analysis = results.get('content_analysis', {})
            print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
            total_keywords = 0
            for url, content in content_analysis.items():
                keywords = content.get('keywords', [])
                total_keywords += len(keywords)
                if keywords:
                    print(f"   {url}: {', '.join([kw.get('word', kw) if isinstance(kw, dict) else str(kw) for kw in keywords[:5]])}")
            print(f"   –í—Å–µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {total_keywords}")
            print()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            stats = results.get('performance_stats', {})
            print(f"‚è±Ô∏è  –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
            print("-" * 40)
            print(f"   –í—Å–µ–≥–æ URL: {stats.get('total_urls', 0)}")
            print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {stats.get('successful_scrapes', 0)}")
            print(f"   –ù–µ—É–¥–∞—á–Ω—ã—Ö: {stats.get('failed_scrapes', 0)}")
            print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats.get('duration', 0):.2f} —Å–µ–∫")
            print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {stats.get('total_urls', 0) / max(stats.get('duration', 1), 0.1):.2f} URL/—Å–µ–∫")
            print()
            
            # –û—à–∏–±–∫–∏
            errors = results.get('errors', {})
            print(f"‚ùå –û–®–ò–ë–ö–ò:")
            print("-" * 40)
            print(f"   –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {errors.get('total_errors', 0)}")
            print(f"   –ó–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö URL: {errors.get('affected_urls', 0)}")
            error_types = errors.get('error_types', {})
            if error_types:
                print("   –¢–∏–ø—ã –æ—à–∏–±–æ–∫:")
                for error_type, count in error_types.items():
                    print(f"     {error_type}: {count}")
            print()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫—Ä–∞–ø–µ—Ä–∞
            scraper_stats = scraper.get_scraping_stats()
            print(f"üîß –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ö–†–ê–ü–ï–†–ê:")
            print("-" * 40)
            print(f"   –ü–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL: {scraper_stats.get('visited_urls_count', 0)}")
            print(f"   –†–∞–∑–º–µ—Ä –∫–µ—à–∞: {scraper_stats.get('cache_size', 0)}")
            print(f"   –î–æ–º–µ–Ω–æ–≤ –≤ rate limiter: {scraper_stats.get('rate_limiter_stats', {}).get('domains', 0)}")
            print()
            
            print("üéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        asyncio.run(demo_enhanced_scraper())
    except KeyboardInterrupt:
        print("\nüõë –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
