#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import asyncio
import sys
import time

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append('/Users/rafaelamirov/Documents/metod/Email-Intelligence-Collector/backend')

from modules.web_scraper import EnhancedWebScraper, ScrapingConfig, SelectorConfig

async def final_demo():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
    
    print("üéØ" * 30)
    print("üöÄ –§–ò–ù–ê–õ–¨–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –£–õ–£–ß–®–ï–ù–ù–û–ì–û –í–ï–ë-–°–ö–†–ê–ü–ï–†–ê")
    print("üéØ" * 30)
    print()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    config = ScrapingConfig(
        max_pages=2,
        concurrent_requests=2,
        timeout=10,
        retry_attempts=1,
        cache_ttl=60,
        delay_between_requests=0.5
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
    selector_config = SelectorConfig()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ URL - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä
    test_urls = [
        "http://localhost:9000/demo.html",
        "http://localhost:9000/"  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ª–∏—Å—Ç–∏–Ω–≥
    ]
    
    email = "john.smith@techcorp.com"
    
    print(f"üìß Email –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {email}")
    print(f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {len(test_urls)} URL")
    print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {config.max_pages}")
    print(f"   ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {config.concurrent_requests}")
    print(f"   ‚Ä¢ –¢–∞–π–º–∞—É—Ç: {config.timeout} —Å–µ–∫")
    print(f"   ‚Ä¢ –í—Ä–µ–º—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è: {config.cache_ttl} —Å–µ–∫")
    print()
    
    try:
        async with EnhancedWebScraper(email, config, selector_config) as scraper:
            print("üîç –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞...")
            start_time = time.time()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–≤—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥
            results = await scraper.scrape_websites_enhanced(test_urls)
            
            print("‚úÖ –ü–µ—Ä–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print()
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print("üìä –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print("‚ïê" * 50)
            
            # 1. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            person_info = results.get('person_info', {})
            print("üë§ –ü–ï–†–°–û–ù–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
            if person_info:
                for key, value in person_info.items():
                    print(f"   ‚úì {key.title()}: {value}")
            else:
                print("   ‚ö†Ô∏è  –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            print()
            
            # 2. –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            contact_info = results.get('contact_info', {})
            print("üìû –ö–û–ù–¢–ê–ö–¢–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
            
            emails = contact_info.get('emails', [])
            print(f"   üìß Email-–∞–¥—Ä–µ—Å–∞ ({len(emails)}):")
            for email_addr in emails[:3]:
                print(f"      ‚Ä¢ {email_addr}")
            
            phones = contact_info.get('phones', [])
            print(f"   üì± –¢–µ–ª–µ—Ñ–æ–Ω—ã ({len(phones)}):")
            for phone in phones[:3]:
                print(f"      ‚Ä¢ {phone}")
            
            addresses = contact_info.get('addresses', [])
            print(f"   üè† –ê–¥—Ä–µ—Å–∞ ({len(addresses)}):")
            for address in addresses[:2]:
                print(f"      ‚Ä¢ {address}")
            print()
            
            # 3. –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏
            social_links = results.get('social_links', [])
            print(f"üîó –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò ({len(social_links)}):")
            for link in social_links:
                platform = link.get('platform', 'Unknown')
                url = link.get('url', 'N/A')
                print(f"   ‚Ä¢ {platform}: {url}")
            print()
            
            # 4. NLP –ê–Ω–∞–ª–∏–∑
            nlp_analysis = results.get('nlp_analysis', {})
            print("üß† NLP –ê–ù–ê–õ–ò–ó:")
            if nlp_analysis:
                for url, analysis in nlp_analysis.items():
                    print(f"   üìÑ URL: {url}")
                    
                    # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                    sentiment = analysis.get('sentiment', {})
                    polarity = sentiment.get('polarity', 0)
                    subjectivity = sentiment.get('subjectivity', 0)
                    
                    polarity_label = "–ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è" if polarity > 0.1 else "–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è" if polarity < -0.1 else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è"
                    subj_label = "—Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è" if subjectivity > 0.5 else "–æ–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è"
                    
                    print(f"   üòä –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {polarity_label} (–ø–æ–ª—è—Ä–Ω–æ—Å—Ç—å: {polarity:.2f})")
                    print(f"   üé≠ –°—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {subj_label} ({subjectivity:.2f})")
                    
                    # –Ø–∑—ã–∫–∏
                    lang = analysis.get('language_detected', 'unknown')
                    print(f"   üåê –Ø–∑—ã–∫: {lang}")
                    
                    # –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
                    entities_spacy = analysis.get('entities_spacy', {})
                    entities_nltk = analysis.get('entities_nltk', {})
                    
                    if entities_spacy or entities_nltk:
                        print(f"   üè∑Ô∏è  –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:")
                        all_entities = {**entities_spacy, **entities_nltk}
                        for entity_type, entity_list in all_entities.items():
                            if entity_list:
                                print(f"      ‚Ä¢ {entity_type}: {', '.join(entity_list[:3])}\")\n            else:\n                print(\"   ‚ö†Ô∏è  NLP –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω\")\n            print()\n            \n            # 5. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n            content_analysis = results.get('content_analysis', {})\n            print(\"üîë –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:\")\n            total_keywords = 0\n            for url, content in content_analysis.items():\n                keywords = content.get('keywords', [])\n                if keywords:\n                    print(f\"   üìÑ {url}:\")\n                    for i, kw in enumerate(keywords[:10]):\n                        if isinstance(kw, dict):\n                            word = kw.get('word', 'N/A')\n                            freq = kw.get('frequency', 0)\n                            pos = ', '.join(kw.get('pos_tags', ['UNKNOWN']))\n                            print(f\"      {i+1}. {word} (—á–∞—Å—Ç–æ—Ç–∞: {freq}, —Ç–∏–ø: {pos})\")\n                        else:\n                            print(f\"      {i+1}. {kw}\")\n                    total_keywords += len(keywords)\n                    print()\n            print(f\"   üìä –í—Å–µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {total_keywords}\")\n            print()\n            \n            # 6. –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n            print(\"üìã –ú–ï–¢–ê–î–ê–ù–ù–´–ï:\")\n            for url, content in content_analysis.items():\n                meta_info = content.get('meta_info', {})\n                if meta_info:\n                    print(f\"   üìÑ {url}:\")\n                    for key, value in list(meta_info.items())[:5]:\n                        print(f\"      ‚Ä¢ {key}: {value}\")\n                    print()\n            \n            # 7. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n            stats = results.get('performance_stats', {})\n            print(\"‚è±Ô∏è  –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:\")\n            print(\"‚îÄ\" * 30)\n            duration = stats.get('duration', 0)\n            total_urls = stats.get('total_urls', 0)\n            successful = stats.get('successful_scrapes', 0)\n            failed = stats.get('failed_scrapes', 0)\n            \n            print(f\"   ‚è∞ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫\")\n            print(f\"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {total_urls}\")\n            print(f\"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {successful}\")\n            print(f\"   ‚ùå –ù–µ—É–¥–∞—á–Ω–æ: {failed}\")\n            print(f\"   üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {total_urls/max(duration, 0.1):.2f} URL/—Å–µ–∫\")\n            print(f\"   üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {successful/max(total_urls, 1)*100:.1f}%\")\n            print()\n            \n            # 8. –û—à–∏–±–∫–∏\n            errors = results.get('errors', {})\n            print(\"üö® –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö:\")\n            total_errors = errors.get('total_errors', 0)\n            if total_errors > 0:\n                print(f\"   ‚ö†Ô∏è  –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {total_errors}\")\n                print(f\"   üåê –ó–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö URL: {errors.get('affected_urls', 0)}\")\n                \n                error_types = errors.get('error_types', {})\n                if error_types:\n                    print(\"   üìä –¢–∏–ø—ã –æ—à–∏–±–æ–∫:\")\n                    for error_type, count in error_types.items():\n                        print(f\"      ‚Ä¢ {error_type}: {count}\")\n            else:\n                print(\"   ‚úÖ –û—à–∏–±–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\")\n            print()\n            \n            # 9. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫—Ä–∞–ø–µ—Ä–∞\n            scraper_stats = scraper.get_scraping_stats()\n            print(\"üîß –í–ù–£–¢–†–ï–ù–ù–Ø–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n            print(\"‚îÄ\" * 30)\n            print(f\"   üåê –ü–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL: {scraper_stats.get('visited_urls_count', 0)}\")\n            print(f\"   üíæ –†–∞–∑–º–µ—Ä –∫–µ—à–∞: {scraper_stats.get('cache_size', 0)} –∑–∞–ø–∏—Å–µ–π\")\n            \n            rate_stats = scraper_stats.get('rate_limiter_stats', {})\n            print(f\"   üö¶ –î–æ–º–µ–Ω–æ–≤ –≤ rate limiter: {rate_stats.get('domains', 0)}\")\n            \n            delays = rate_stats.get('current_delays', {})\n            if delays:\n                print(\"   ‚è≥ –¢–µ–∫—É—â–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏:\")\n                for domain, delay in delays.items():\n                    print(f\"      ‚Ä¢ {domain}: {delay:.2f} —Å–µ–∫\")\n            print()\n            \n            # 10. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è\n            print(\"üíæ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ö–ï–®–ò–†–û–í–ê–ù–ò–Ø:\")\n            print(\"–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫–µ—à–∞...\")\n            \n            cache_start = time.time()\n            cached_results = await scraper.scrape_websites_enhanced([test_urls[0]])\n            cache_duration = time.time() - cache_start\n            \n            cache_stats = cached_results.get('performance_stats', {})\n            cache_hits = cache_stats.get('cache_hits', 0)\n            \n            print(f\"   ‚ö° –í—Ä–µ–º—è —Å –∫–µ—à–µ–º: {cache_duration:.2f} —Å–µ–∫\")\n            print(f\"   üìä –ü–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫–µ—à: {cache_hits}\")\n            print(f\"   üöÄ –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {duration/max(cache_duration, 0.01):.1f}x\")\n            print()\n            \n            execution_time = time.time() - start_time\n            print(\"üèÅ –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê:\")\n            print(\"‚ïê\" * 50)\n            print(f\"‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")\n            print(f\"‚è∞ –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f} —Å–µ–∫—É–Ω–¥\")\n            print(f\"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö:\")\n            print(f\"   ‚Ä¢ –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {len(person_info)} –ø–æ–ª–µ–π\")\n            print(f\"   ‚Ä¢ –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(emails + phones + addresses)} –∑–∞–ø–∏—Å–µ–π\")\n            print(f\"   ‚Ä¢ –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏: {len(social_links)} –ø–ª–∞—Ç—Ñ–æ—Ä–º\")\n            print(f\"   ‚Ä¢ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {total_keywords} —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n            print(f\"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã: –í–´–°–û–ö–ê–Ø\")\n            print()\n            print(\"üéâ –£–ª—É—á—à–µ–Ω–Ω—ã–π –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –≤—Å–µ –∑–∞—è–≤–ª–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏!\")\n            \n    except Exception as e:\n        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}\")\n        import traceback\n        traceback.print_exc()\n\ndef main():\n    \"\"\"–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\"\"\"\n    try:\n        asyncio.run(final_demo())\n    except KeyboardInterrupt:\n        print(\"\\nüõë –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\")\n    except Exception as e:\n        print(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n\nif __name__ == \"__main__\":\n    main()
