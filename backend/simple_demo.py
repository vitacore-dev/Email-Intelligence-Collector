#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞
"""

import asyncio
import sys
import time

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append('/Users/rafaelamirov/Documents/metod/Email-Intelligence-Collector/backend')

from modules.web_scraper import EnhancedWebScraper, ScrapingConfig, SelectorConfig

async def simple_demo():
    """–ü—Ä–æ—Å—Ç–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
    
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –£–õ–£–ß–®–ï–ù–ù–û–ì–û –í–ï–ë-–°–ö–†–ê–ü–ï–†–ê")
    print("=" * 60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    config = ScrapingConfig(
        max_pages=1,
        concurrent_requests=1,
        timeout=10,
        retry_attempts=1,
        cache_ttl=60,
        delay_between_requests=0.5
    )
    
    selector_config = SelectorConfig()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π URL
    test_urls = ["http://localhost:9000/demo.html"]
    email = "john.smith@techcorp.com"
    
    print(f"üìß Email: {email}")
    print(f"üîó URL: {test_urls[0]}")
    print()
    
    try:
        async with EnhancedWebScraper(email, config, selector_config) as scraper:
            print("üîç –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞...")
            start_time = time.time()
            
            results = await scraper.scrape_websites_enhanced(test_urls)
            
            duration = time.time() - start_time
            print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.2f} —Å–µ–∫")
            print()
            
            # –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            person_info = results.get('person_info', {})
            print("üë§ –ü–ï–†–°–û–ù–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
            if person_info:
                for key, value in person_info.items():
                    print(f"   {key}: {value}")
            else:
                print("   –ù–µ –Ω–∞–π–¥–µ–Ω–∞")
            print()
            
            # –ö–æ–Ω—Ç–∞–∫—Ç—ã
            contact_info = results.get('contact_info', {})
            emails = contact_info.get('emails', [])
            phones = contact_info.get('phones', [])
            addresses = contact_info.get('addresses', [])
            
            print("üìû –ö–û–ù–¢–ê–ö–¢–´:")
            print(f"   Email: {len(emails)} –Ω–∞–π–¥–µ–Ω–æ")
            for email_addr in emails[:3]:
                print(f"      ‚Ä¢ {email_addr}")
            print(f"   –¢–µ–ª–µ—Ñ–æ–Ω—ã: {len(phones)} –Ω–∞–π–¥–µ–Ω–æ")
            for phone in phones[:3]:
                print(f"      ‚Ä¢ {phone}")
            print(f"   –ê–¥—Ä–µ—Å–∞: {len(addresses)} –Ω–∞–π–¥–µ–Ω–æ")
            for address in addresses[:2]:
                print(f"      ‚Ä¢ {address}")
            print()
            
            # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏
            social_links = results.get('social_links', [])
            print(f"üîó –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò: {len(social_links)} –Ω–∞–π–¥–µ–Ω–æ")
            for link in social_links:
                platform = link.get('platform', 'Unknown')
                url = link.get('url', 'N/A')
                print(f"   ‚Ä¢ {platform}: {url}")
            print()
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            content_analysis = results.get('content_analysis', {})
            print("üîë –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:")
            total_keywords = 0
            for url, content in content_analysis.items():
                keywords = content.get('keywords', [])
                total_keywords += len(keywords)
                if keywords:
                    print(f"   –¢–æ–ø-10 —Å–ª–æ–≤:")
                    for i, kw in enumerate(keywords[:10]):
                        if isinstance(kw, dict):
                            word = kw.get('word', 'N/A')
                            freq = kw.get('frequency', 0)
                            print(f"      {i+1}. {word} (—á–∞—Å—Ç–æ—Ç–∞: {freq})")
                        else:
                            print(f"      {i+1}. {kw}")
            print(f"   –í—Å–µ–≥–æ: {total_keywords} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            print()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = results.get('performance_stats', {})
            print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"   –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {stats.get('successful_scrapes', 0)}")
            print(f"   –ù–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {stats.get('failed_scrapes', 0)}")
            print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats.get('duration', 0):.2f} —Å–µ–∫")
            
            # –û—à–∏–±–∫–∏
            errors = results.get('errors', {})
            print(f"   –û—à–∏–±–æ–∫: {errors.get('total_errors', 0)}")
            print()
            
            # –¢–µ—Å—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
            print("üíæ –¢–ï–°–¢ –ö–ï–®–ò–†–û–í–ê–ù–ò–Ø:")
            cache_start = time.time()
            cached_results = await scraper.scrape_websites_enhanced(test_urls)
            cache_duration = time.time() - cache_start
            
            print(f"   –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: {duration:.2f} —Å–µ–∫")
            print(f"   –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {cache_duration:.2f} —Å–µ–∫")
            print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {duration/max(cache_duration, 0.01):.1f}x")
            print()
            
            print("üéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            print("‚úÖ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

def main():
    try:
        asyncio.run(simple_demo())
    except KeyboardInterrupt:
        print("\nüõë –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
